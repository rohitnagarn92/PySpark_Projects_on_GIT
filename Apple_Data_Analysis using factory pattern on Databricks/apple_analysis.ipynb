{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdb0137-e56c-4d3c-8dd6-cbc15753828f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./reader_factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a87fa09-f875-44cc-8463-66bcad5ca78e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73919c9c-4ab5-4a05-9d85-83f209fd1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb85a9c-0690-430c-9b07-2ce5d83cca03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./loader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ec24ac-f01c-4288-934f-59386e9988fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./loader_factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f62398-c4e1-45f8-962b-797b61964508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FirstWorkFlow:\n",
    "    \"\"\"\n",
    "    ETL PIPELINE to generate the data for all customers who have bought Airpods after buying Iphone\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        # Step 1: Extract all required data from different source\n",
    "        inputDF = AirpodsAfterIphoneExtractor().extract()\n",
    "        #print(inputDF)\n",
    "\n",
    "        # Step 2: Implement Transformation logic\n",
    "        # Customers who bought Airpods after buying the iPhone\n",
    "        firstTransform = FirstTransformer().transform(inputDF)\n",
    "        #firstTransform.show()\n",
    "\n",
    "        # Step 3: Load all required data to different sink\n",
    "        AirPodsAfterIphoneLoader(firstTransform).sink()\n",
    "\n",
    "        # Read the result parquet file\n",
    "        print(\"result data set is\")\n",
    "        \"\"\"\n",
    "        The error message indicates that you are trying to read a Delta table using the Parquet format, \n",
    "        but Delta tables are not the same as regular Parquet files. \n",
    "        Delta tables use the Parquet file format, but they also maintain a transaction log for ACID transactions. \n",
    "        Since a Delta table was detected, you must use the Delta format instead of Parquet.\n",
    "        \"\"\"\n",
    "        resultDF = spark.read.format(\"delta\").load('dbfs:/FileStore/tables/apple_analysis/output/airpodsAfterIphone/')\n",
    "        resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e46795-435b-4f91-90b1-b331721be298",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SecondWorkFlow:\n",
    "    \"\"\"\n",
    "    ETL PIPELINE to generate the data for all customers who have bought Airpods after buying Iphone\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        # Step 1: Extract all required data from different source\n",
    "        inputDF = AirpodsAndIphoneExtractor().extract()\n",
    "        #print(inputDF)\n",
    "\n",
    "        # Step 2: Implement Transformation logic\n",
    "        # Customers who bought Airpods after buying the iPhone\n",
    "        firstTransform = OnlyAirpodsAndIphone().transform(inputDF)\n",
    "        firstTransform.show()\n",
    "\n",
    "        # Step 3: Load all required data to different sink\n",
    "        AirPodsAndIphoneLoader(firstTransform).sink()\n",
    "\n",
    "        # Read the result parquet file\n",
    "        print(\"result data set is\")\n",
    "        \"\"\"\n",
    "        The error message indicates that you are trying to read a Delta table using the Parquet format, \n",
    "        but Delta tables are not the same as regular Parquet files. \n",
    "        Delta tables use the Parquet file format, but they also maintain a transaction log for ACID transactions. \n",
    "        Since a Delta table was detected, you must use the Delta format instead of Parquet.\n",
    "        \"\"\"\n",
    "        resultDF = spark.read.format(\"delta\").load('dbfs:/FileStore/tables/apple_analysis/output/airpodsAndIphone/')\n",
    "        resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1598caa9-2aa5-4267-8b98-87af62a611f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n|customer_id|            products|\n+-----------+--------------------+\n|        107|   [AirPods, iPhone]|\n|        108|   [AirPods, iPhone]|\n|        106|[AirPods, iPhone,...|\n|        105|[AirPods, iPhone,...|\n+-----------+--------------------+\n\nfilteredDF having only AirPods and IPhones\n+-----------+-----------------+\n|customer_id|         products|\n+-----------+-----------------+\n|        107|[AirPods, iPhone]|\n|        108|[AirPods, iPhone]|\n+-----------+-----------------+\n\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        107|        Grace|Colorado|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\nresult data set is\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        107|        Grace|Colorado|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "class WorkFlowRunner:\n",
    "    \"\"\"\n",
    "    ETL PIPELINE to generate the data for all customers who have bought Airpods after buying Iphone\n",
    "    \"\"\"\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        if self.name ==\"firstworkflow\":\n",
    "            return FirstWorkFlow().runner()\n",
    "        elif self.name ==\"secondworkflow\":\n",
    "            return SecondWorkFlow().runner()\n",
    "        else:\n",
    "            raise ValueError(\"Not Implemented\")\n",
    "        \n",
    "name1 = \"firstworkflow\"\n",
    "name2 = \"secondworkflow\"\n",
    "\n",
    "workflowrunner = WorkFlowRunner(name2).runner()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "apple_analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
